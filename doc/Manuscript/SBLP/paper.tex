\documentclass{paper}
\usepackage{sbc-template}

\usepackage{proof}
\usepackage{mathpartir}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}

\def\qed{\unskip\kern 10pt{\unitlength1pt\linethickness{.4pt}\framebox(6,6){}}}

\newcommand{\fun}[1]{\mbox{\textbf{#1}}}
\newcommand{\lb}[1]{#1_{\downarrow}}
\newcommand{\ub}[1]{#1_{\uparrow}}
\newcommand{\varset}[1]{\mbox{$\cal{#1}$}}

\sloppy

\begin{document}

\title{Speed and Precision in Range Analysis}

\author{Victor Hugo Sperle Campos, Igor Rafael de Assis Costa,\\
Raphael Ernani Rodrigues and Fernando Magno Quint\~{a}o Pereira}

\address{UFMG -- 6627 Ant\^{o}nio Carlos Av, 31.270-010, Belo Horizonte, Brazil
\email{\{victorsc,raphael.ernani,igor.rafael,fernando\}@dcc.ufmg.br}
}


\maketitle

%mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
\begin{abstract}
Range analysis is a compiler technique that determines statically the lower and
upper values that each integer variable from a target program may assume
during this program's execution.
This type of inference is very important, because it enables several compiler
optimizations, such as dead and redundant code elimination, bitwidth aware
register allocation, and detection of program vulnerabilities.
In this paper we describe an inter-procedural, context-sensitive range analysis
algorithm that we have implemented in the LLVM compiler.
During the effort to produce an industrial-quality implementation of our
algorithm, we had to face a constant tension between precision and speed.
The foremost goal of this paper is to discuss the many engineering choices
that, due to this tension, have shaped our implementation.
Given the breath of our evaluation, we believe that this paper
contains the most comprehensive empirical study of a range analysis
algorithm ever presented in the compiler related literature.
\end{abstract}

%mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
\section{Introduction}
\label{sec:int}

% What range analysis is good for
Range analysis is a compiler technique whose objective is to determine
statically, for each program variable, limits for the minimum and maximum
values that this variable might assume during the program execution.
Range analysis is important because it enables many compiler optimizations.
Among these optimizations, the most well-known are dead and redundant
code elimination.
Examples of redundant code elimination include the removal of array bounds
checks~\cite{Bodik00,Logozzo08,Venet04} and overflow checks~\cite{Sol11}.
Additionally, range analysis is also used in bitwidth aware register
allocation~\cite{Barik06,Pereira08,Tallam03}, branch
prediction~\cite{Patterson95} and synthesis of hardware for specific
applications~\cite{Cong05,Lhairech10,Mahlke01,Stephenson00}.
Because of this importance, the programming language community has put much
effort in the design and implementation of efficient and precise range
analysis algorithms.

% The lack of a comprehensive empirical evaluation
However, the compiler related literature does not contain a comprehensive
evaluation of range analysis algorithms that scale up to entire programs.
Many works on this subject are limited to very small
programs~\cite{Mahlke01,Simon08,Stephenson00}, or, given their theoretic
perspective, have never been implemented in production
compilers~\cite{Costan05,Gawlitza09,Lakhdar11,Su04,Su05}.
There are implementations of range analysis that deal with very large
programs~\cite{Bertrane10,Cousot09,Logozzo08,Oh11}; nevertheless, because
these papers focus on applications of range analysis,
and not on its implementation, they do not provide a thorough discussion
about their engineering decisions.
A noticeable exception is the recent work of Oh {\em et al.}~\cite{Oh12},
which discusses a range analysis algorithm developed for C programs that can
handle very large benchmarks.
Oh {\em et al.} present an evaluation of the speed and memory consumption of
their implementation.
In this paper we claim to push this discussion considerably further.

% Summary of our experiments
We have implemented an industrial-quality range analysis algorithm in the
LLVM compiler~\cite{Lattner04}.
As we show in Section~\ref{sub:showdown}, our implementation, which is
publicly available, is able to analyze programs with over one million assembly
instructions in ten seconds.
And our implementation is not a straw-man: it produces very precise results.
We have compared the results produced by our implementation with the results
obtained via a dynamic profiler, which we have also implemented.
As we show in Section~\ref{sub:showdown}, when analyzing well-known numeric
benchmarks we are able to estimate tight ranges for almost half of all the
integer variables present in these programs.

% Design space exploration:
While designing and implementing our algorithm we had to face several important
engineering choices.
Many approaches that we have used in an attempt to increase the precision of
our implementation would result in runtime slowdowns.
Although we cannot determine the optimum spot in this design space, given the
vast number of possibilities, we discuss our most important implementation
decisions in Section~\ref{sec:design}.
Section~\ref{sub:sccs} shows how we could improve runtime and precision
substantially by processing data-flow information in the strongly connected
components that underly our constraint system.
Section~\ref{sub:program_rep} discuss the importance of choosing a suitable
intermediate representation when implementing a sparse data-flow framework.
Section~\ref{sub:whole} compares the intra-procedural and the inter-procedural
versions of our algorithm.
The role of context sensitiveness is discussed in Section~\ref{sub:context}.
Finally, Section~\ref{sub:widen} discusses the different widening strategies
that we have experimented with.

% The history of our implementation
This work concludes a two years long effort to produce a solid and scalable
implementation of range analysis.
Our first endeavor to implement such an algorithm was based on Su and
Wagner's constraint system, which can be solved exactly in polynomial
time~\cite{Su04,Su05}.
However, although we could use their formulation to handle a subset of C-like
constructs, their description of how to deal with loops was
not very explicit.
Thus, in order to solve loops we adopted Gawlitza
{\em et al.}'s~\cite{Gawlitza09} approach.
This technique uses the Bellman-Ford algorithm to detect increasing or
decreasing cycles in the constraint system, and then saturates these cycles
via a simple widening operator.
A detailed description of our implementation has been published by
Couto and Pereira~\cite{Couto11}.
Nevertheless, the inability to handle comparisons between variables, and the
cubic complexity of the Bellman-Ford method eventually led us to seek
alternative solutions to range analysis.
This quest reached a pinnacle in the present work, which we summarize in this
paper.

\section{Brief Description of our Range Analysis Algorithm}
\label{sec:desc}

% Define the lattice, the constraints and the valuation I.
\noindent
\textbf{The Interval Lattice.}
Following Gawlitza {\em et al.}'s notation, we shall be performing arithmetic
operations over the complete lattice
$\cal{Z} = \mathbb{Z} \cup \{-\infty, +\infty\}$, where the ordering is
naturally given by $-\infty < \ldots < -2 < -1 < 0 < 1 < 2 < \ldots +\infty$.
For any $x > -\infty$ we define:

\begin{tabular}{lcl}
$x + \infty = \infty, x \neq -\infty$ & \mbox{\hspace{0.1cm}} & $x - \infty = - \infty, x \neq +\infty$ \\
$x \times \infty = \infty$ if $x > 0$ & & $x \times \infty = -\infty$ if $x < 0$ \\
$0 \times \infty = 0$ & & $(-\infty) \times \infty = \ $ not defined $$ \\
\end{tabular}

From the lattice $\varset{Z}$ we define the product lattice
$\varset{Z}^2$, which is partially ordered by the subset relation
$\sqsubseteq$.
$\varset{Z}^2$ is defined as follows:
%
\begin{equation*}
\varset{Z}^2 = \emptyset \cup \{[z_1, z_2] | \ z_1,z_2 \in \varset{Z},
\ z_1 \leq z_2, \  -\infty < z_2 \}
\end{equation*}
%
The objective of range analysis is to determine a mapping
$I: \varset{V} \mapsto \varset{Z}^2$ from the set of integer program variables
$V$ to intervals, such that, for any variable $v \in V$, if
$I(v) = [l, u]$, then, during the execution of the target program, any
valued $i$ assigned to $v$ is such that $l \leq i \leq u$.

\noindent
\textbf{A Holistic View of our Range Analysis Algorithm.}
We perform range analysis in a number of steps.
First, we convert the program to a suitable intermediate representation that
makes it easier to extract constraints.
From these constraints, we build a dependence graph that allows us to do
range analysis sparsely.
Finally, we solve the constraints applying different fix-point iterators on
this dependence graph.
Figure~\ref{fig:algorithm} gives a global view of this algorithm.
Some of the steps in the algorithm are optional.
They improve the precision of the range analysis, at the expense of a longer
running time.
In Section~\ref{sec:design} we discuss in more details these tradeoffs.
The last phase, which we call the {\em micro algorithm}, happens per
strong component; however, if we opted for not building these components,
then it happens once for the entire constraint graph.
Nevertheless, the use of strongly connected components
is so essential for performance and precision, as we show in
Section~\ref{sub:sccs}, that it is considered optional only because we
can easily build our implementation without this module.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/algorithm}
\end{center}
\caption{\label{fig:algorithm}
Our implementation of range analysis. Rounded boxes are optional steps.}
\end{figure}

We will illustrate the mandatory parts of the
algorithm via the example program in Figure~\ref{fig:overall_view}.
More details about each phase of the algorithm will be introduced in
Section~\ref{sec:design}, when we discuss our engineering decisions.
Figure~\ref{fig:overall_view}(a) shows an example program taken from the
partition function of the quicksort algorithm used by Bodik
{\em et al.}~\cite{Bodik00}.
We have removed the code that performs array manipulation from this program,
as it plays no role in our explanation.
Figure~\ref{fig:overall_view}(b) shows one possible way to represent this
program internally.
As we explain in Section~\ref{sub:program_rep}, a good program
representation allows us to find more precise results.
In this example we chose a program representation called
Extended Static Single Assignment form, which lets us to solve range
analysis via a path sensitive algorithm.
Figure~\ref{fig:overall_view}(c) shows the constraints that we extract from
the intermediate representation seen in part (b) of this figure.
From these constraints we build the {\em constraint graph} in
Figure~\ref{fig:overall_view}(d).
This graph is the main data-structure that we use to solve range analysis.
For each variable $v$ in the constraint system, the constraint graph has a node
$n_v$.
Similarly, for each constraint $v = f(\ldots, u, \ldots)$ in the constraint
system, the graph has an {\em operation node} $n_f$.
For each constraint $v = f(\ldots, u, \ldots)$ we add two edges to the
graph: $\overrightarrow{n_un_f}$ and $\overrightarrow{n_fn_v}$.
Some edges in the constraint graph are dashed.
These are called {\em control dependence edges}.
If a constraint $v = f(\ldots, \fun{ft}(u), \ldots)$ uses a {\em future}
bound from a variable $u$, then we add to the constraint graph a control
dependence edge $\overrightarrow{n_un_f}$.
The final solution to this instance of the range analysis problem is
given in Figure~\ref{fig:overall_view}(e).

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/overall_view}
\end{center}
\caption{\label{fig:overall_view}
Range analysis by example.
(a) Input program.
(b) Internal compiler representation.
(c) Constraints of the range analysis problem.
(d) The constraint graph.
(e) The final solution.}
\end{figure}

\noindent
\textbf{The Micro Algorithm.}
We find the solution given in Figure~\ref{fig:overall_view}(e) in a
process that we call the micro algorithm.
This phase is further divided into three sub-steps:
(i) growth analysis;
(ii) future resolution and
(iii) narrowing analysis.

\noindent
\textbf{Growth analysis. }
The objective of growth analysis is to determine the growth behavior of
each program variable.
There are four possible behaviors:
(a) the variable is bound to a constant interval,
such as $k_0$ in Figure~\ref{fig:overall_view}(b).
(b) The variable is bound to a decreasing interval, i.e., an interval whose
lower bound decreases.
This is the case of $j_1$ in our example.
(c) The variable is bound to an increasing interval, i.e., its upper bound
increases.
This is the case of $i_1$ in the example.
(d) The variable is bound to an interval that expands in both directions.
The growth analysis uses an infinite lattice, i.e., $\varset{Z}^2$.
Thus, a careless implementation of an algorithm that infers growth patterns
might not terminate.
In order to ensure termination, we must reckon on a technique called
{\em widening}, first introduced by Cousot and Cousot as a key component
of abstract interpretation~\cite{Cousot77}.
There are many different widening strategies.
We discuss some of them in Section~\ref{sub:widen}.

\noindent
\textbf{Future resolution. }
In order to learn information from comparisons between variables, such as
\texttt{i < j} in Figure~\ref{fig:overall_view}(a), we bind some intervals
to {\em futures}.
Futures are symbolic limits, which will be replaced by actual numbers once
we finish the growth analysis.
The ranges found by the growth analysis tells us which variables have fixed
bounds, independent on the intersections in the constraint system.
Thus, we can use actual limits to replace intersections bounded by futures.
Figure~\ref{fig:fix_intersects} shows the rules to perform these substitutions.
In order to correctly replace a future $\fun{ft}(V)$ that limits a variable
$V'$, we need to have already applied the growth analysis onto $V$.
Had we considered only data dependence edges, then it would be possible
that $V'$ be analyzed before $V$.
However, because of control dependence edges, this case cannot happen.
The control dependence edges ensure that any topological ordering of the
constraint graph either places $N_v$ before $N_{v'}$, or places these nodes
in the same strongly connected component.
For instance, in Figure~\ref{fig:overall_view}(b), variables $j_1$ and $i_t$
are in the same SCC only because of the control dependence edges.

\begin{figure}[t!]
\begin{center}
\begin{eqnarray*}
\begin{array}{c}
\inferrule{Y = X \sqcap [l, \fun{ft}(V) + c] \\ \ub{I[V]} = u}
{Y = X \sqcap [l, u + c]} \mbox{\hspace{0.3cm}} u, c \in \mathbb{Z} \cup \{-\infty, +\infty\}
\\
\\
\inferrule{Y = X \sqcap [\fun{ft}(V) + c, u] \\ \lb{I[V]} = l}
{Y = X \sqcap [l + c, u]} \mbox{\hspace{0.3cm}} l, c \in \mathbb{Z} \cup \{-\infty, +\infty\}
\end{array}
\end{eqnarray*}
\end{center}
\caption{\label{fig:fix_intersects}Rules to replace futures by actual
bounds. $S$ is the interval bound to each variable after the widening
analysis.}
\end{figure}

\noindent
\textbf{Narrowing analysis.}
The growth analysis associates very conservative bounds to each variable.
Thus, the last step of our algorithm consists in narrowing these intervals.
We accomplish this step via Cousot and Cousot's classic narrowing
operator~\cite[248]{Cousot77}, which we show in
Figure~\ref{fig:crop_analysis}.

\begin{figure}[t!]
\begin{center}
\begin{eqnarray*}
\begin{array}{c@{\hspace{0.9cm}}c}
\inferrule{\lb{I[Y]} = -\infty \\ \lb{e(Y)} > -\infty}
{I[Y] \leftarrow [\lb{e(Y)}, \ub{I[Y]}]}
&
\inferrule{\lb{I[Y]} > \lb{e(Y)}}
{I[Y] \leftarrow [\lb{e(Y)}, \ub{I[Y]}]}
\\
\\
\inferrule{\ub{I[Y]} = +\infty \\ \ub{e(Y)} < +\infty}
{I[Y] \leftarrow [\lb{I[Y]}, \ub{e(Y)}]}
&
\inferrule{\ub{I[Y]} < \ub{e(Y)}}
{I[Y] \leftarrow [\lb{I[Y]}, \ub{e(Y)}]}
\end{array}
\end{eqnarray*}
\end{center}
\caption{\label{fig:crop_analysis}Cousot and Cousot's narrowing operator.}
\end{figure}

\noindent
\textbf{Example.}
Continuing with our example, Figure~\ref{fig:ex_partition_grow_crop} shows
the application of our algorithm on the last strong component of
Figure~\ref{fig:overall_view}(d).
Upon meeting this SCC, we have already determined that the interval
$[0, 0]$ is bound to $i_0$ and that the interval $[100, 100]$ is bound to
$j_0$.
We are not guaranteed to find the least fix point of a constraint system.
However, in this example we did it.
We emphasize that finding this tight solution was only possible because of
the topological ordering of the constraint graph in
Figure~\ref{fig:overall_view}(d).
Had we applied the widening operator onto the whole graph, then we would
have found out that variable $j_0$ is bound to $[-\infty, +\infty]$,
because
(i) it receives its interval directly from variable $k_t$, which is upper
bounded by $+\infty$, and
(ii) it is part of a negative cycle.
On the other hand, by only analyzing $j$'s SCC after we have
analyzed $k$'s, $k$ only contribute the constant range $[0, 99]$ to $j_0$.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/ex_partition_grow_crop}
\end{center}
\caption{\label{fig:ex_partition_grow_crop}
Four snapshots of the last SCC of Figure~\ref{fig:overall_view}(d).
(a) After removing control dependence edges.
(b) After running the growth analysis.
(c) After fixing the intersections bound to futures.
(d) After running the narrowing analysis.}
\end{figure}

\subsection{Range Analysis Showdown}
\label{sub:showdown}

The objective of this section is to show, via experimental numbers, that our
implementation of range analysis is fast, economic and effective.
We have used it to analyze a test suite with 2.72 million lines of C code,
which includes, in addition to all the benchmarks distributed with LLVM,
the programs in the SPEC CPU 2006 collection.

\noindent
\textbf{Time and Memory Complexity}

Figure~\ref{fig:TimeCorr} provides a visual comparison between the time to
run our algorithm and the size of the input programs.
We show data for the 100 largest benchmarks in our test suite, in number
of variable nodes in the constraint graph.
We perform function inlining before running our analysis, to increase program
sizes.
Each point in the X line corresponds to a benchmark.
We analyze the smallest benchmark in this set, \texttt{Prolangs-C/deriv2}, which
has 1,131 variable nodes in the constraint graph, in 20ms.
We take 15,91secs to analyze our largest benchmark, \texttt{403.gcc}, which,
after function inlining, has 1,266,273 assembly instructions, and a
constraint graph with 679,652 variable nodes.
For this data set, the coefficient of determination $(R^2)$ is 0.967, which
provides very strong evidence about the linear asymptotic complexity of our
implementation.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\textwidth]{images/TimeCorr}
\end{center}
\caption{\label{fig:TimeCorr}
Correlation between program size (number of var nodes in constraint
graphs after inlining) and analysis runtime (ms).
Coefficient of determination = 0.967.
}
\end{figure}

The experiments also reveal that the memory consumption of our implementation
is linear with the program size.
Figure~\ref{fig:MemCorr} plots these two quantities together.
The linear correlation, in this case, is even stronger than that found in
Figure~\ref{fig:TimeCorr}, which compares runtime and program size: the
coefficient of determination is 0.9947.
The figure only shows our 100 largest benchmarks.
Again, SPEC \texttt{403.gcc} is the heaviest benchmark, requiring
265,588KB to run.
Memory includes stack, heap and the executable program code.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.9\textwidth]{images/MemCorr}
\end{center}
\caption{\label{fig:MemCorr}
Comparison between program size (number of var nodes in constraint
graphs) and memory consumption (KB).
Coefficient of determination = 0.9947.
}
\end{figure}

\noindent
\textbf{Precision}

Our implementation of range analysis is remarkably precise, considering its
runtime.
Lakhdar {\em et al.}'s relational analysis~\cite{Lakhdar11}, for instance, takes
about 25 minutes to go over a program with almost 900 basic blocks.
We analyze programs of similar size in less than one second.
We do not claim our approach is as precise as such algorithms, even though we
are able to find exact bounds to 4/5 of the examples presented
in~\cite{Lakhdar11}.
On the contrary, this paper presents a compromise between precision and speed
that scales to very large programs.
Nevertheless, our results are far from being trivial.
We have implemented a dynamic profiler that measures, for each variable,
its upper and lower limits, given an execution of the target program.
Figure~\ref{fig:precision} compares our results with those measured
dynamically for the Stanford benchmark suite, which is publicly
available~\footnote{\texttt{http://classes.engineering.wustl.edu/cse465/docs/BCCExamples/stanford.c}}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/precUpperBound}
\includegraphics[width=\textwidth]{images/precLowerBound}
\end{center}
\caption{\label{fig:precision}
(Upper) Comparison between static range analysis and dynamic profiler for
upper bounds.
(Lower) Comparison between static range analysis and dynamic profiler for
lower bounds. The numbers above the benchmark names give the number of
variables in each program.}
\end{figure}

We have classified the bounds estimated by the static analysis into four
categories.
The first category, which we call $1$, contains those bounds that are tight:
during the execution of the program, the variable has been assigned an upper,
or lower limit, that equals the limit inferred statically.
The second category, which we call $n$, contains the estimated bounds that are
within twice the value inferred statically.
For instance, if the range analysis estimates that a variable $v$ is in the
range $[0, 100]$, and during the execution the dynamic profiler finds that
its maximum value is $51$, then $v$ falls into this category.
The third category, $n^2$, contains variables whose actual value is within
a quadratic factor from the estimated value.
In our example, $v$'s upper bound would have to be at most $10$ for it to
be in this category.
Finally, the fourth category contains variables whose estimated value lays
outside a quadratic factor of the actual value.
We call this category {\em imprecise}, and it contains mostly the limits that
our static analysis has marked as either $+\infty$ or $-\infty$.

As we see in Figure~\ref{fig:precision}, 54.11\% of the lower limits that
we have estimated statically are exact.
Similarly, 51.99\% of our upper bounds are also tight.
The figure also shows that, on average, 37.39\% of our lower limits are
imprecise, and 35.40\% of our upper limits are imprecise.
This result is on pair with those obtained by more costly analysis, such as
Stephenson {\em et al.}'s~\cite{Stephenson00}.
However, whereas that approach have not been used with programs larger than
the Stanford benchmark suite, we, as shown before, have been able to
deal with remarkably larger programs.

%mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
\section{Design Space}
\label{sec:design}

As we see from a cursory glance at Figure~\ref{fig:algorithm}, our range
analysis algorithm has many optional modules.
These modules provide the user with the possibility to choose between
more precise results, or a faster analysis.
Given the number of options, the design space of a range analysis algorithm
is vast.
In this section we try to cover some of the most important tradeoffs.
Figure~\ref{fig:space} plots, for the integer programs in the SPEC CPU 2006
benchmark suite, precision versus speed for different configurations of
our implementation.
Our initial goal when developing this analysis was to support a bitwidth-aware
register allocator.
Thus, we measure precision by the average number of bits that our
analysis allows us to save per program variables.
It is very important to notice that we do not consider constants in our statistics of precision.
In other words, we only measure bitwidth reduction in variables that a constant
propagation step could not remove.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/space}
\end{center}
\caption{\label{fig:space}
Design space exploration: precision (percentage of bitwidth reduction)
versus speed (secs) for different configurations of our algorithm analyzing
the SPEC CPU 2006 integer benchmarks.}
\end{figure}

\subsection{Strongly Connected Components}
\label{sub:sccs}

The greatest source of improvement in our implementation is the use of strongly
connected components.
In order to propagate ranges across the constraint graph, we fragment it
into strongly connected components, collapse each of these components into
single nodes, and sort the resulting directed acyclic graph topologically.
We then solve the range analysis problem for each component individually.
Once we have solved a component, we propagate its ranges to the next
components, and repeat the process until we walk over the entire constraint
graph.
It is well-known that this technique is essential to speedup constraint solving
algorithms~\cite[Sec 6.3]{Nielson99}.
In our case, the results are dramatic, mostly in terms of speed, but also in
terms of precision.
Figure~\ref{fig:impactSCC} shows the speed up that we gain by using strong
components.
We show results for the integer programs in the SPEC CPU 2006 benchmark suite.
In some cases, as in \texttt{xalancbmk} the analysis on strong components is
450x faster.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/impactSCC}
\end{center}
\caption{\label{fig:impactSCC}
(Left) Bars give time to run our analysis without building strong components
divided by time to run the analysis on strongly connected components.
(Right) Bars give precision, in bitwidth reduction, that we obtain with strong
components, divided by the precision that we obtain without them.
}
\end{figure}

The strong components improve the precision of our growth analysis.
As we see in Figure~\ref{fig:impactSCC}, in some cases, as in \texttt{bzip2},
it is 40\% more precise.
The gains in precision happen because, by completely resolving a component,
we are able to propagate constant intervals to the next components, instead
of propagating intervals that can grow in both directions.
An an example, in Figure~\ref{fig:ex_partition_grow_crop} we pass the range
$[0, 99]$ from variable $k$ to the component that contains variable $j$.
Had we run the analysis in the entire constraint graph, by the time we
applied the growth analysis on $j$ we would still find $k$ bound to
$0, +\infty$.

\subsection{Intra versus Inter-procedural Analysis}
\label{sub:whole}

A naive implementation of range analysis would be intra-procedural; that is,
would solve the range analysis problem once for each function.
However, we can gain in precision by performing it inter-procedurally.
An inter-procedural implementation allows the results found for a function $f$
to flow into other functions that $f$ calls.
Figure~\ref{fig:intra} illustrates the inter-procedural analysis for the
program seen in Figure~\ref{fig:overall_view}(a).
The trivial way to produce an inter-procedural implementation is to insert
into the constraint system assignments from the actual parameter names to the
formal parameter names.
In our example of Figure~\ref{fig:intra}, our constraint graph contains a flow
of information from $0$, the actual parameter, to $k_0$, the formal parameter
of function \texttt{foo}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/intra}
\end{center}
\caption{\label{fig:intra}
Example where an intra-procedural implementation would lead to imprecise
results.
}
\end{figure}

Figure~\ref{fig:wholeImpact} shows that the whole program analysis increases
moderately the precision of our analysis.
We show results for the five largest programs in three different categories of
benchmarks: SPEC CPU 2006, the Stanford Suite~\footnote{\texttt{http://classes.engineering.wustl.edu/cse465/docs/BCCExamples/stanford.c}} and
Bitwise~\cite{Stephenson00}.
Our results for the SPEC programs were disappointingly: on the average for
the five largest programs, the intra-procedural version of our analysis saves
5.23\% of the total program bitwidth.
By using the inter-procedural version we can increase this number to 8.89\%.
A manual inspection of the SPEC programs reveal that this result is expected:
they manipulate files, and their texts do no provide enough
explicit constants to power our analysis up.
However, with numerical benchmarks we fare much better.
On the average our inter-procedural algorithm reduces the bitwidth of the
Stanford benchmarks by 36.24\%.
Finally, for Bitwise we obtain a bitwidth reduction of 12.27\%.
However, this average is brought down by two outliers: \texttt{edge\_detect} and
\texttt{sha}, which cannot be reduced.
The bitwise benchmarks were implemented by Stephenson
{\em et al.}~\cite{Stephenson00} to validate their bitwidth analysis.
Our results are on par with those found by the original authors.
The bitwise programs contain only the \texttt{main} function; thus, different
versions of our algorithm find the same results.

\subsection{Context Sensitive versus Context Insensitive Analysis}
\label{sub:context}

Another way to increase the precision of range analysis is by using a
context-sensitive implementation.
Context-sensitiveness allows us to distinguish different calling sites of
the same function.
Figure~\ref{fig:context} shows why the ability to make this distinction is
important for precision.
In Figure~\ref{fig:context}(a) we have two different calls of function
\texttt{foo}.
An usual way to perform a data-flow analysis inter-procedurally is to
create assignments between formal and actual parameters, as we show in
Figure~\ref{fig:context}(b).
However, in this case the multiple assignment of values to parameters
makes the ranges of these parameters very large, whereas in reality they
are not.
A way to circumvent this source of imprecision is via function inlining,
as we show in Figure~\ref{fig:context}(c).
The results that we can derive for the transformed program are more
precise, as each input parameter is assigned a single value.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/context}
\end{center}
\caption{\label{fig:context}
Example where a context-sensitive implementation improves the results of
range analysis.
}
\end{figure}

Figure~\ref{fig:wholeImpact} also shows how function inlining modifies the
precision of our results.
It is difficult to find an adequate way to compare the precision of
our analysis with, and without inlining.
This difficulty stems from the fact that this transformation tends to change
the target program too much.
In absolute numbers, we always reduce the bitwidth of more variables after
function inlining.
However, proportionally function inlining leads to a smaller percentage of
bitwidth reduction for many benchmarks.
In the Stanford Collection, for instance, where most of the functions are
called in only one location, inlining leads to worst precision results.
On the other hand, for the SPEC programs, inlining, even in terms of
percentage of reduction, tends to increase our measure of precision.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{images/wholeImpact}
\end{center}
\caption{\label{fig:wholeImpact}
The impact of whole program analysis on precision. Each bar gives precision in
\%bitwidth reduction.}
\end{figure}

\noindent
\textbf{Intra vs Inter-procedural runtimes.}
Figure~\ref{fig:timeComp}(Right) compares three different execution modes.
Bars are normalized to the time to run the intra-procedural analysis
without inlining.
On the average, the intra-procedural mode is 28.92\% faster than the
inter-procedural mode.
If we perform function inlining, then this difference is larger: 45.87\%.
These numbers are close because we run our analysis on strong components.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.45\textwidth]{images/timeIntraInterInline}
\end{center}
\caption{\label{fig:timeComp}
Runtime comparison between intra, inter and inter+inline versions of
our algorithm.
The bars are normalized to the time to run the intra-procedural analysis.
}
\end{figure}


\subsection{The choice of a program representation}
\label{sub:program_rep}

If strong components account for the largest gains in speed, the choice of
a suitable program representation is responsible for the largest gains in
precision.
However, here we no longer have a win-win condition: a more expressive
program representation decreases our speed, because it increases the
size of the target program.
We have tried our analysis in two different program representations: the
Static Single Assignment (SSA) Form~\cite{Cytron91}, and the Extended Static
Single Assignment (e-SSA) form~\cite{Bodik00}.
The SSA form gives us a faster, albeit more imprecise, analysis.
Any program in e-SSA form has also the SSA core property: any variable name
has at most one definition site.
The contrary is not true: SSA form programs do not have the core e-SSA
property: any use site of a variable that appears in a conditional test
post-dominates its definition.
The program in Figure~\ref{fig:overall_view}(b) is in e-SSA form.
The live ranges of variables $i_1$ and $j_1$ have been split right after the
conditional test via the assertions that created variables $i_t$ and $j_t$.
The e-SSA format serves well analyses that extract information from definition
sites and conditional tests, and propagate this information forwardly.
Examples include, in addition to range analysis, tainted flow
analysis~\cite{Rimsa11} and array bounds checks elimination~\cite{Bodik00}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1\textwidth]{images/impactESSA}
\end{center}
\caption{\label{fig:impactESSA}
(Left) Bars give the time to run analysis on e-SSA form programs divided by
the time to run analysis on SSA form programs.
(Right) Bars give the size of the e-SSA form program, in number of
assembly instructions, divided by the size of the SSA form program.}
\end{figure}

Figure~\ref{fig:impactESSA} compares these two program representations in
terms of runtime.
As we see in Figure~\ref{fig:impactESSA}(Left), the e-SSA form slows down our
analysis.
In some cases, as in \texttt{xalancbmk}, this slowdown increases execution
time by 71\%.
Runtime increases because the e-SSA form programs are larger than the SSA
form programs, as we show in Figure~\ref{fig:impactESSA}(Right).
However, this growth is small: in none of the integer programs in SPEC
CPU 2006 we verified an increase in code size of more than 9\%.
But, if the e-SSA form slowdowns the analysis runtime, its gains in precision
are remarkable, as we show in Figure~\ref{fig:precESSA}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1\textwidth]{images/precESSA}
\end{center}
\caption{\label{fig:precESSA}
The impact of the e-SSA transformation on precision for three different
benchmark suites. Bars give the ratio of precision (in bitwidth reduction),
obtained with e-SSA form conversion divided by precision without e-SSA form
conversion.}
\end{figure}

\subsection{Choosing a Widening Strategy}
\label{sub:widen}

We have implemented the widening operator used in the growth analysis in
two different ways.
The first way, which we call {\em simple}, is based on Cousot and
Cousot's original widening operator~\cite{Cousot77}.
This operator is shown in Figure~\ref{fig:widening}, and it is the one
used in Figure~\ref{fig:ex_partition_grow_crop}(b).
The second widening strategy, which we call {\em jump-set widening} consists
in using the constants that appear in the program text, in sorted order, as
the next limits of each interval after widening is applied.
This operator is common in implementations of range
analysis~\cite[p.228]{Nielson99}.
There are situations in which jump-set widening produces better results
than the simple operator.
Figure~\ref{fig:jumpSet} shows an example taken from the code of
SPEC CPU \texttt{bzip2}.
Part of the constraint graph of the program in Figure~\ref{fig:jumpSet}(a)
is given in Figure~\ref{fig:jumpSet}(b).
The result of applying the simple operator is shown in
Figure~\ref{fig:jumpSet}(c).
Jump-set widening would use the lattice in Figure~\ref{fig:jumpSet}(d),
instead of the lattice in Figure~\ref{fig:widening}(Right).
This lattice would yield the result given in Figure~\ref{fig:jumpSet}(e),
which is more precise.

\begin{figure}[t!]
\begin{center}
\begin{tabular}{c@{\hspace{1.5cm}}c}
\begin{minipage}{2cm}
\includegraphics{images/growth_lattice}
\end{minipage}
&
\begin{minipage}{8cm}
\begin{small}
\begin{eqnarray*}
\begin{array}{c@{\hspace{0.5cm}}c}
\inferrule{I[Y] = [\bot, \bot]}{I[Y] \leftarrow e(Y)}
&
\inferrule{\lb{e(Y)} < \lb{I[Y]} \\ \ub{e(Y)} > \ub{I[Y]}}
{I[Y] \leftarrow [-\infty, +\infty ]}
\\
\\
\inferrule{\lb{e(Y)} < \lb{I[Y]}}
{I[Y] \leftarrow [-\infty, \ub{I[Y]}]}
&
\inferrule{\ub{e(Y)} > \ub{I[Y]}}
{I[Y] \leftarrow [\lb{I[Y]}, +\infty]}
\end{array}
\end{eqnarray*}
\end{small}
\end{minipage}
\end{tabular}
\end{center}
\caption{\label{fig:widening}
(Left) The lattice used in the simple widening strategy.
(Right) Cousot and Cousot's widening operator. We evaluate the rules from
left-to-right, top-to-bottom, and stop upon finding a pattern matching.
Again: given an interval $\iota = [l, u]$, we let $\lb{\iota} = l$, and
$\ub{\iota} = u$}
\end{figure}


\begin{figure}[t!]
\begin{center}
\includegraphics[width=1\textwidth]{images/jumpSet}
\end{center}
\caption{\label{fig:jumpSet}
An example where jump-set widening is more precise.}
\end{figure}

Another way to improve the precision of growth analysis is to perform a few
rounds of abstract interpretation on the constraint graph, and, in case the
process does not reach a fix point, only then applying the widening operator.
Each round consists in evaluating all the constraints, and updating the
intervals that change from one evaluation to the other.
The simple widening strategy, coupled with one round of abstract interpretation
would give us precise results in the example from Figure~\ref{fig:jumpSet}.
We have experimented with 0 and 16 iterations before doing widening, and the
overall result, for the programs in the SPEC CPU 2006 suite is given in
Figure~\ref{fig:space}.
Figure~\ref{fig:wideningPrec} shows some of these results in more details
for a broader set of benchmarks.
In general jump-set widening improves the precision of our results in
non-trivial ways.
Nevertheless, the simple widening operator preceded by 16 rounds of
abstract interpretation in general is more precise than jump-set widening
without any cycle of pre-evaluation, as we see in Figure~\ref{fig:space}.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=1\textwidth]{images/wideningPrec}
\end{center}
\caption{\label{fig:wideningPrec}
The impact of the widening strategy on precision.
Each bar gives precision in \%bitwidth reduction.}
\end{figure}


%mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
\section{Conclusion}
\label{sec:con}

This paper presents what we believe is the most comprehensive evaluation of
range analysis in the literature.
Altogether we have experimented with 32 different configurations of our
range analysis algorithm.
Our implementation is
publicly available at \texttt{http://code.google.com/p/range-analysis/}.
This repository contains instructions about how to deploy and use our
implementation.
We provide a gallery of examples, including source codes,
CFGs and constraint graphs that we produce for meaningful programs at
\texttt{http://code.google.com/p/range-analysis/wiki/gallery}.

\bibliographystyle{plain}
\bibliography{../references}

\end{document}
